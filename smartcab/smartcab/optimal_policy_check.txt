/opt/anaconda27/bin/python2.7 /home/jon/dev/learn/smartcab/smartcab/optimal_policy_check.py
State: ('left', 'forward', 'left', 'red', 'left') Optimal action: None Q: {'forward': -40.16, 'right': 1.16, None: 2.02, 'left': -38.74} Agent optimal? True
State: ('forward', None, 'forward', 'red', 'forward') Optimal action: None Q: {'forward': -39.73, 'right': -19.93, None: 1.84, 'left': -40.04} Agent optimal? True
State: ('left', 'forward', 'forward', 'red', 'forward') Optimal action: None Q: {'forward': -39.95, 'right': 0.63, None: 1.62, 'left': -39.12} Agent optimal? True
State: ('left', 'right', 'forward', 'red', 'left') Optimal action: None Q: {'forward': -6.32, 'right': 0.85, None: 0.0, 'left': -9.87} Agent optimal? False
State: ('right', 'left', 'left', 'red', 'left') Optimal action: None Q: {'forward': -10.1, 'right': 0.5, None: 2.0, 'left': -8.76} Agent optimal? True
State: (None, None, 'right', 'red', 'left') Optimal action: None Q: {'forward': -9.82, 'right': 0.53, None: 1.77, 'left': -40.09} Agent optimal? True
State: ('right', 'left', 'right', 'red', 'forward') Optimal action: None Q: {'forward': -4.21, 'right': 0.34, None: 0.0, 'left': 0.0} Agent optimal? False
State: ('left', 'forward', None, 'red', 'left') Optimal action: None Q: {'forward': -40.2, 'right': 1.0, None: 1.8, 'left': -39.58} Agent optimal? True
State: (None, 'left', 'left', 'red', 'right') Optimal action: right Q: {'forward': -10.46, 'right': 1.59, None: 1.69, 'left': -10.23} Agent optimal? False
State: ('right', 'right', None, 'red', 'right') Optimal action: right Q: {'forward': 0.0, 'right': 2.07, None: 1.45, 'left': -6.34} Agent optimal? True
State: ('forward', 'left', None, 'red', 'right') Optimal action: None Q: {'forward': -39.55, 'right': -19.88, None: 1.89, 'left': -40.24} Agent optimal? True
State: (None, 'left', 'forward', 'red', 'forward') Optimal action: None Q: {'forward': -10.37, 'right': 0.93, None: 1.83, 'left': -10.12} Agent optimal? True
State: ('forward', 'forward', None, 'red', 'right') Optimal action: None Q: {'forward': -39.66, 'right': -20.62, None: 1.8, 'left': -40.06} Agent optimal? True
State: ('forward', 'forward', 'left', 'red', 'forward') Optimal action: None Q: {'forward': -37.81, 'right': -19.5, None: 1.92, 'left': -30.29} Agent optimal? True
State: ('left', 'right', None, 'red', 'forward') Optimal action: None Q: {'forward': -9.91, 'right': 0.4, None: 1.82, 'left': -9.96} Agent optimal? True
State: ('right', 'right', 'forward', 'red', 'forward') Optimal action: None Q: {'forward': -7.74, 'right': 0.13, None: 1.99, 'left': 0.0} Agent optimal? True
State: ('left', 'left', 'right', 'red', 'forward') Optimal action: None Q: {'forward': 0.0, 'right': 0.0, None: 1.57, 'left': -26.11} Agent optimal? True
State: ('forward', 'forward', 'right', 'red', 'forward') Optimal action: None Q: {'forward': -10.85, 'right': -19.2, None: 1.72, 'left': -33.49} Agent optimal? True
State: ('forward', 'forward', 'forward', 'red', 'right') Optimal action: None Q: {'forward': -31.0, 'right': 0.0, None: 1.83, 'left': -40.23} Agent optimal? True
State: ('left', 'forward', 'right', 'red', 'forward') Optimal action: None Q: {'forward': -39.7, 'right': 0.85, None: 1.73, 'left': 0.0} Agent optimal? True
State: ('right', 'forward', 'forward', 'red', 'forward') Optimal action: None Q: {'forward': -5.92, 'right': 0.69, None: 0.0, 'left': 0.0} Agent optimal? False
State: (None, 'forward', None, 'red', 'forward') Optimal action: None Q: {'forward': -40.28, 'right': 0.49, None: 1.65, 'left': -40.0} Agent optimal? True
State: ('left', 'right', 'left', 'red', 'right') Optimal action: right Q: {'forward': -9.1, 'right': 2.01, None: 1.05, 'left': -9.28} Agent optimal? True
State: ('right', 'left', 'right', 'red', 'left') Optimal action: None Q: {'forward': -9.93, 'right': 0.0, None: 0.71, 'left': -38.62} Agent optimal? True
State: ('left', 'left', 'right', 'red', 'right') Optimal action: right Q: {'forward': -9.11, 'right': 1.8, None: 1.2, 'left': -37.91} Agent optimal? True
State: ('left', 'right', 'forward', 'red', 'right') Optimal action: right Q: {'forward': -9.57, 'right': 1.75, None: 1.02, 'left': -8.67} Agent optimal? True
State: (None, 'right', 'right', 'red', 'left') Optimal action: None Q: {'forward': -8.81, 'right': 0.82, None: 1.71, 'left': -38.65} Agent optimal? True
State: ('left', None, None, 'red', 'forward') Optimal action: None Q: {'forward': -9.83, 'right': 0.94, None: 2.07, 'left': -10.2} Agent optimal? True
State: ('left', 'left', None, 'red', 'forward') Optimal action: None Q: {'forward': -10.37, 'right': 0.82, None: 2.0, 'left': -10.1} Agent optimal? True
State: (None, 'left', 'forward', 'red', 'right') Optimal action: right Q: {'forward': -10.14, 'right': 2.03, None: 1.22, 'left': -10.31} Agent optimal? True
State: (None, 'forward', 'right', 'red', 'forward') Optimal action: None Q: {'forward': -39.99, 'right': 0.88, None: 1.74, 'left': -40.07} Agent optimal? True
State: ('left', 'left', 'left', 'red', 'left') Optimal action: None Q: {'forward': -9.9, 'right': 0.56, None: 1.86, 'left': -9.78} Agent optimal? True
State: ('forward', 'left', 'forward', 'red', 'forward') Optimal action: None Q: {'forward': -22.42, 'right': -20.28, None: 1.96, 'left': -28.89} Agent optimal? True
State: ('left', 'forward', 'forward', 'red', 'left') Optimal action: None Q: {'forward': -35.61, 'right': 0.62, None: 1.56, 'left': -39.48} Agent optimal? True
State: ('left', 'right', None, 'red', 'left') Optimal action: None Q: {'forward': -9.59, 'right': 1.2, None: 1.98, 'left': -9.92} Agent optimal? True
State: ('right', None, None, 'red', 'right') Optimal action: right Q: {'forward': -10.08, 'right': 1.74, None: 1.69, 'left': -9.79} Agent optimal? True
State: ('right', 'left', 'left', 'red', 'right') Optimal action: right Q: {'forward': -9.93, 'right': 1.85, None: 1.38, 'left': -7.84} Agent optimal? True
State: ('left', 'forward', 'forward', 'red', 'right') Optimal action: right Q: {'forward': -33.13, 'right': 1.91, None: 0.0, 'left': -10.15} Agent optimal? True
State: ('right', 'left', None, 'red', 'forward') Optimal action: None Q: {'forward': -7.78, 'right': -0.28, None: 1.89, 'left': -10.29} Agent optimal? True
State: (None, 'right', 'right', 'red', 'right') Optimal action: right Q: {'forward': -10.2, 'right': 2.02, None: 1.82, 'left': -31.26} Agent optimal? True
State: (None, 'right', None, 'red', 'left') Optimal action: None Q: {'forward': -9.81, 'right': 0.94, None: 1.72, 'left': -10.14} Agent optimal? True
State: ('left', 'left', 'left', 'red', 'forward') Optimal action: None Q: {'forward': -8.13, 'right': 0.86, None: 0.0, 'left': -5.69} Agent optimal? False
State: ('right', 'right', None, 'red', 'forward') Optimal action: None Q: {'forward': -9.76, 'right': 0.0, None: 1.94, 'left': -6.71} Agent optimal? True
State: ('left', 'left', 'right', 'red', 'left') Optimal action: None Q: {'forward': -9.09, 'right': 1.02, None: 0.54, 'left': -35.57} Agent optimal? False
State: ('right', 'right', 'forward', 'red', 'left') Optimal action: None Q: {'forward': 0.0, 'right': 0.31, None: 1.71, 'left': -7.08} Agent optimal? True
State: ('left', None, 'forward', 'red', 'forward') Optimal action: None Q: {'forward': -9.85, 'right': 0.72, None: 1.82, 'left': -9.78} Agent optimal? True
State: ('left', 'forward', None, 'red', 'forward') Optimal action: None Q: {'forward': -39.78, 'right': 0.79, None: 1.61, 'left': -40.06} Agent optimal? True
State: ('left', 'left', None, 'red', 'right') Optimal action: right Q: {'forward': -10.0, 'right': 1.59, None: 1.93, 'left': -10.14} Agent optimal? False
State: ('forward', 'left', 'right', 'red', 'right') Optimal action: None Q: {'forward': -36.35, 'right': -19.96, None: 1.68, 'left': -40.53} Agent optimal? True
State: ('forward', 'left', 'left', 'red', 'forward') Optimal action: None Q: {'forward': -39.23, 'right': -19.97, None: 1.72, 'left': -7.69} Agent optimal? True
State: (None, 'left', 'forward', 'red', 'left') Optimal action: None Q: {'forward': -10.16, 'right': 0.48, None: 1.95, 'left': -10.33} Agent optimal? True
State: ('forward', 'right', 'left', 'red', 'forward') Optimal action: None Q: {'forward': -39.44, 'right': 0.0, None: 1.83, 'left': -31.06} Agent optimal? True
State: ('left', 'left', 'forward', 'red', 'forward') Optimal action: None Q: {'forward': -9.34, 'right': 0.0, None: 1.96, 'left': -9.51} Agent optimal? True
State: ('right', None, 'left', 'red', 'right') Optimal action: right Q: {'forward': -9.84, 'right': 1.45, None: 1.75, 'left': -10.33} Agent optimal? False
State: (None, 'forward', 'right', 'red', 'left') Optimal action: None Q: {'forward': -39.83, 'right': 0.46, None: 1.87, 'left': -39.95} Agent optimal? True
State: ('right', None, None, 'red', 'left') Optimal action: None Q: {'forward': -10.27, 'right': 1.05, None: 2.24, 'left': -10.11} Agent optimal? True
State: ('forward', 'forward', 'forward', 'red', 'left') Optimal action: None Q: {'forward': -39.56, 'right': -20.5, None: 1.82, 'left': -39.65} Agent optimal? True
State: ('forward', 'right', None, 'red', 'left') Optimal action: None Q: {'forward': -39.68, 'right': -17.87, None: 2.0, 'left': -39.75} Agent optimal? True
State: ('forward', None, None, 'red', 'left') Optimal action: None Q: {'forward': -40.13, 'right': -19.8, None: 1.94, 'left': -40.23} Agent optimal? True
State: ('right', 'forward', 'right', 'red', 'right') Optimal action: right Q: {'forward': 0.0, 'right': 0.55, None: 1.67, 'left': 0.0} Agent optimal? False
State: ('forward', 'left', None, 'red', 'left') Optimal action: None Q: {'forward': -39.91, 'right': -19.55, None: 1.71, 'left': -40.49} Agent optimal? True
State: ('right', 'forward', 'right', 'red', 'forward') Optimal action: None Q: {'forward': 0.0, 'right': 0.0, None: 1.85, 'left': 0.0} Agent optimal? True
State: ('forward', 'forward', None, 'red', 'forward') Optimal action: None Q: {'forward': -39.79, 'right': -19.81, None: 1.82, 'left': -39.86} Agent optimal? True
State: ('forward', None, 'left', 'red', 'forward') Optimal action: None Q: {'forward': -40.01, 'right': -19.78, None: 1.96, 'left': -39.88} Agent optimal? True
State: ('forward', 'left', 'left', 'red', 'right') Optimal action: None Q: {'forward': -39.77, 'right': -19.28, None: 1.97, 'left': -39.89} Agent optimal? True
State: ('left', None, 'right', 'red', 'left') Optimal action: None Q: {'forward': -10.25, 'right': 0.57, None: 2.04, 'left': -39.66} Agent optimal? True
State: (None, 'right', 'left', 'red', 'left') Optimal action: None Q: {'forward': -10.05, 'right': 1.01, None: 1.87, 'left': -9.78} Agent optimal? True
State: ('right', 'left', 'right', 'red', 'right') Optimal action: right Q: {'forward': -9.42, 'right': 1.81, None: 0.0, 'left': -30.49} Agent optimal? True
State: ('forward', 'right', 'forward', 'red', 'right') Optimal action: None Q: {'forward': -35.01, 'right': -4.56, None: 1.95, 'left': -31.91} Agent optimal? True
State: ('forward', 'forward', 'right', 'red', 'left') Optimal action: None Q: {'forward': -39.92, 'right': -15.86, None: 1.8, 'left': -34.38} Agent optimal? True
State: ('left', None, None, 'red', 'right') Optimal action: right Q: {'forward': -9.72, 'right': 1.94, None: 1.38, 'left': -9.7} Agent optimal? True
State: (None, None, 'right', 'red', 'forward') Optimal action: None Q: {'forward': -10.37, 'right': 0.96, None: 1.84, 'left': -39.8} Agent optimal? True
State: ('forward', None, None, 'red', 'forward') Optimal action: None Q: {'forward': -40.26, 'right': -19.77, None: 1.9, 'left': -40.51} Agent optimal? True
State: ('right', 'forward', 'forward', 'red', 'right') Optimal action: right Q: {'forward': -13.36, 'right': 2.58, None: 1.29, 'left': 0.0} Agent optimal? True
State: ('left', 'forward', 'left', 'red', 'right') Optimal action: right Q: {'forward': -39.02, 'right': 1.39, None: 1.76, 'left': -29.4} Agent optimal? False
State: ('forward', 'left', None, 'red', 'forward') Optimal action: None Q: {'forward': -39.33, 'right': -19.97, None: 1.84, 'left': -40.01} Agent optimal? True
State: ('forward', 'right', 'right', 'red', 'left') Optimal action: None Q: {'forward': -26.38, 'right': -2.59, None: 0.19, 'left': -4.93} Agent optimal? True
State: (None, None, None, 'red', 'right') Optimal action: right Q: {'forward': -10.52, 'right': 1.9, None: 1.55, 'left': -9.79} Agent optimal? True
State: (None, 'right', 'left', 'red', 'right') Optimal action: right Q: {'forward': -10.0, 'right': 1.76, None: 1.85, 'left': -10.25} Agent optimal? False
State: ('right', 'right', 'right', 'red', 'left') Optimal action: None Q: {'forward': -2.59, 'right': -0.01, None: 0.67, 'left': -12.46} Agent optimal? True
State: ('right', 'left', None, 'red', 'left') Optimal action: None Q: {'forward': -10.51, 'right': 0.67, None: 1.89, 'left': -9.89} Agent optimal? True
State: ('forward', None, 'left', 'red', 'right') Optimal action: None Q: {'forward': -40.03, 'right': -20.62, None: 1.74, 'left': -40.23} Agent optimal? True
State: ('forward', 'right', None, 'red', 'right') Optimal action: None Q: {'forward': -39.67, 'right': -19.47, None: 1.92, 'left': -40.06} Agent optimal? True
State: ('right', None, 'right', 'red', 'forward') Optimal action: None Q: {'forward': -6.3, 'right': 0.82, None: 1.8, 'left': -40.66} Agent optimal? True
State: (None, 'forward', 'left', 'red', 'left') Optimal action: None Q: {'forward': -39.97, 'right': 0.72, None: 1.93, 'left': -40.2} Agent optimal? True
State: ('right', 'left', 'forward', 'red', 'right') Optimal action: right Q: {'forward': -6.91, 'right': 1.7, None: 0.62, 'left': -9.38} Agent optimal? True
State: (None, 'left', None, 'red', 'right') Optimal action: right Q: {'forward': -9.49, 'right': 1.95, None: 1.25, 'left': -9.97} Agent optimal? True
State: ('forward', 'left', 'forward', 'red', 'right') Optimal action: None Q: {'forward': -39.65, 'right': -16.73, None: 1.8, 'left': -38.67} Agent optimal? True
State: ('left', 'right', 'left', 'red', 'left') Optimal action: None Q: {'forward': -9.89, 'right': 0.27, None: 1.88, 'left': -8.85} Agent optimal? True
State: (None, None, 'forward', 'red', 'right') Optimal action: right Q: {'forward': -10.37, 'right': 1.51, None: 1.91, 'left': -9.82} Agent optimal? False
State: (None, 'left', 'right', 'red', 'right') Optimal action: right Q: {'forward': -9.52, 'right': 1.19, None: 1.86, 'left': -32.82} Agent optimal? False
State: ('right', None, None, 'red', 'forward') Optimal action: None Q: {'forward': -10.12, 'right': 0.44, None: 1.99, 'left': -10.12} Agent optimal? True
State: ('left', None, 'forward', 'red', 'left') Optimal action: None Q: {'forward': -10.18, 'right': 0.46, None: 1.86, 'left': -9.76} Agent optimal? True
State: ('left', 'forward', 'right', 'red', 'left') Optimal action: None Q: {'forward': -39.6, 'right': 0.45, None: 2.04, 'left': -19.62} Agent optimal? True
State: ('left', None, 'left', 'red', 'forward') Optimal action: None Q: {'forward': -9.92, 'right': 0.6, None: 1.87, 'left': -10.17} Agent optimal? True
State: ('right', 'right', None, 'red', 'left') Optimal action: None Q: {'forward': -9.79, 'right': 0.75, None: 0.18, 'left': -9.6} Agent optimal? False
State: (None, None, 'left', 'red', 'right') Optimal action: right Q: {'forward': -9.87, 'right': 1.65, None: 1.92, 'left': -10.04} Agent optimal? False
State: (None, 'forward', 'left', 'red', 'forward') Optimal action: None Q: {'forward': -39.63, 'right': 0.59, None: 1.89, 'left': -39.78} Agent optimal? True
State: ('right', None, 'right', 'red', 'left') Optimal action: None Q: {'forward': -8.47, 'right': 1.13, None: 1.71, 'left': -39.95} Agent optimal? True
State: ('forward', 'forward', 'forward', 'red', 'forward') Optimal action: None Q: {'forward': -12.38, 'right': -15.4, None: 1.34, 'left': -31.22} Agent optimal? True
State: ('right', 'forward', 'right', 'red', 'left') Optimal action: None Q: {'forward': 0.0, 'right': 0.39, None: 0.0, 'left': -33.01} Agent optimal? False
State: (None, 'right', 'forward', 'red', 'left') Optimal action: None Q: {'forward': -9.55, 'right': 0.51, None: 1.94, 'left': -9.76} Agent optimal? True
State: (None, None, 'left', 'red', 'forward') Optimal action: None Q: {'forward': -9.88, 'right': 0.89, None: 1.91, 'left': -10.02} Agent optimal? True
State: ('forward', 'forward', 'left', 'red', 'right') Optimal action: None Q: {'forward': -29.3, 'right': 0.0, None: 1.89, 'left': -38.13} Agent optimal? True
State: (None, 'forward', 'forward', 'red', 'left') Optimal action: None Q: {'forward': -40.31, 'right': 1.14, None: 1.85, 'left': -39.88} Agent optimal? True
State: ('forward', None, 'right', 'red', 'left') Optimal action: None Q: {'forward': -37.65, 'right': -19.88, None: 1.98, 'left': -36.12} Agent optimal? True
State: ('left', 'right', 'forward', 'red', 'forward') Optimal action: None Q: {'forward': 0.0, 'right': 0.39, None: 1.69, 'left': -9.02} Agent optimal? True
State: ('right', None, 'forward', 'red', 'right') Optimal action: right Q: {'forward': -9.65, 'right': 0.44, None: 1.81, 'left': -9.99} Agent optimal? False
State: ('forward', None, 'forward', 'red', 'left') Optimal action: None Q: {'forward': -39.86, 'right': -19.84, None: 1.98, 'left': -40.17} Agent optimal? True
State: ('right', 'forward', 'left', 'red', 'left') Optimal action: None Q: {'forward': -37.86, 'right': 0.14, None: 1.65, 'left': -37.16} Agent optimal? True
State: ('forward', None, 'right', 'red', 'forward') Optimal action: None Q: {'forward': -39.19, 'right': -20.13, None: 2.08, 'left': -36.28} Agent optimal? True
State: (None, None, None, 'red', 'left') Optimal action: None Q: {'forward': -10.19, 'right': 0.74, None: 1.83, 'left': -9.95} Agent optimal? True
State: (None, 'forward', None, 'red', 'right') Optimal action: right Q: {'forward': -40.09, 'right': 1.33, None: 2.12, 'left': -40.09} Agent optimal? False
State: ('forward', 'right', 'forward', 'red', 'forward') Optimal action: None Q: {'forward': -39.27, 'right': -8.69, None: 1.22, 'left': -12.97} Agent optimal? True
State: (None, 'left', 'right', 'red', 'left') Optimal action: None Q: {'forward': -10.08, 'right': 0.35, None: 1.97, 'left': -38.83} Agent optimal? True
State: ('forward', None, 'forward', 'red', 'right') Optimal action: None Q: {'forward': -40.22, 'right': -19.92, None: 1.88, 'left': -40.06} Agent optimal? True
State: (None, None, 'left', 'red', 'left') Optimal action: None Q: {'forward': -10.21, 'right': 0.85, None: 1.92, 'left': -10.18} Agent optimal? True
State: ('left', 'left', 'forward', 'red', 'left') Optimal action: None Q: {'forward': -4.42, 'right': 1.32, None: 1.96, 'left': -9.75} Agent optimal? True
State: ('right', 'right', 'left', 'red', 'right') Optimal action: right Q: {'forward': 0.0, 'right': 1.11, None: 1.76, 'left': -7.74} Agent optimal? False
State: (None, 'right', 'forward', 'red', 'forward') Optimal action: None Q: {'forward': -9.97, 'right': 0.43, None: 2.02, 'left': -9.83} Agent optimal? True
State: (None, None, 'forward', 'red', 'left') Optimal action: None Q: {'forward': -10.02, 'right': 1.19, None: 1.93, 'left': -9.76} Agent optimal? True
State: ('left', None, 'right', 'red', 'forward') Optimal action: None Q: {'forward': -9.78, 'right': 0.3, None: 1.77, 'left': -40.46} Agent optimal? True
State: ('left', 'left', 'forward', 'red', 'right') Optimal action: right Q: {'forward': -9.38, 'right': 1.61, None: 0.91, 'left': -9.8} Agent optimal? True
State: ('forward', 'forward', 'left', 'red', 'left') Optimal action: None Q: {'forward': -38.16, 'right': 0.0, None: 1.91, 'left': -32.8} Agent optimal? True
State: ('right', 'left', 'forward', 'red', 'forward') Optimal action: None Q: {'forward': 0.0, 'right': 0.36, None: 1.89, 'left': -7.98} Agent optimal? True
State: ('forward', 'right', 'left', 'red', 'right') Optimal action: None Q: {'forward': 0.0, 'right': -7.68, None: 1.89, 'left': -31.62} Agent optimal? True
State: ('left', 'forward', None, 'red', 'right') Optimal action: right Q: {'forward': -39.47, 'right': 1.43, None: 2.03, 'left': -40.1} Agent optimal? False
State: (None, 'left', 'left', 'red', 'left') Optimal action: None Q: {'forward': -9.98, 'right': 0.79, None: 1.84, 'left': -10.17} Agent optimal? True
State: ('forward', 'forward', None, 'red', 'left') Optimal action: None Q: {'forward': -40.11, 'right': -20.2, None: 1.85, 'left': -40.27} Agent optimal? True
State: ('right', 'forward', None, 'red', 'left') Optimal action: None Q: {'forward': -39.91, 'right': -0.06, None: 1.7, 'left': -40.0} Agent optimal? True
State: (None, 'right', 'left', 'red', 'forward') Optimal action: None Q: {'forward': -9.55, 'right': 0.61, None: 2.09, 'left': -9.9} Agent optimal? True
State: ('right', None, 'right', 'red', 'right') Optimal action: right Q: {'forward': -9.65, 'right': 0.46, None: 1.24, 'left': -40.07} Agent optimal? False
State: ('left', 'right', 'right', 'red', 'left') Optimal action: None Q: {'forward': 0.0, 'right': 0.0, None: 2.16, 'left': -28.06} Agent optimal? True
State: ('left', None, 'left', 'red', 'right') Optimal action: right Q: {'forward': -9.75, 'right': 1.96, None: 1.11, 'left': -9.73} Agent optimal? True
State: ('right', 'left', None, 'red', 'right') Optimal action: right Q: {'forward': -9.54, 'right': 1.91, None: 1.66, 'left': -9.43} Agent optimal? True
State: ('left', 'right', 'right', 'red', 'right') Optimal action: right Q: {'forward': -7.26, 'right': 0.49, None: 0.0, 'left': 0.0} Agent optimal? True
State: ('right', 'forward', 'left', 'red', 'right') Optimal action: right Q: {'forward': 0.0, 'right': 1.38, None: 1.85, 'left': 0.0} Agent optimal? False
State: ('right', 'right', 'left', 'red', 'forward') Optimal action: None Q: {'forward': -7.95, 'right': 0.97, None: 2.06, 'left': 0.0} Agent optimal? True
State: (None, 'left', None, 'red', 'forward') Optimal action: None Q: {'forward': -9.67, 'right': 0.69, None: 1.96, 'left': -9.99} Agent optimal? True
State: ('right', 'right', 'right', 'red', 'forward') Optimal action: None Q: {'forward': 0.0, 'right': -0.05, None: 0.0, 'left': -8.9} Agent optimal? False
State: ('forward', 'left', 'right', 'red', 'forward') Optimal action: None Q: {'forward': -40.05, 'right': -19.98, None: 1.6, 'left': 0.0} Agent optimal? True
State: ('right', 'left', 'left', 'red', 'forward') Optimal action: None Q: {'forward': -8.41, 'right': 0.6, None: 1.79, 'left': -7.14} Agent optimal? True
State: (None, 'forward', 'forward', 'red', 'forward') Optimal action: None Q: {'forward': -39.87, 'right': 1.06, None: 1.83, 'left': -39.51} Agent optimal? True
State: ('right', 'forward', 'left', 'red', 'forward') Optimal action: None Q: {'forward': -28.24, 'right': 0.98, None: 0.75, 'left': -17.71} Agent optimal? False
State: ('left', 'forward', 'left', 'red', 'forward') Optimal action: None Q: {'forward': -39.89, 'right': 0.87, None: 2.07, 'left': -38.53} Agent optimal? True
State: ('right', None, 'left', 'red', 'forward') Optimal action: None Q: {'forward': -9.86, 'right': 0.53, None: 2.15, 'left': -9.83} Agent optimal? True
State: ('forward', 'left', 'left', 'red', 'left') Optimal action: None Q: {'forward': -39.58, 'right': -19.97, None: 2.01, 'left': -38.66} Agent optimal? True
State: ('left', None, None, 'red', 'left') Optimal action: None Q: {'forward': -9.77, 'right': 0.86, None: 1.9, 'left': -9.81} Agent optimal? True
State: ('forward', 'right', 'right', 'red', 'forward') Optimal action: None Q: {'forward': -37.67, 'right': -5.16, None: 0.68, 'left': 0.0} Agent optimal? True
State: ('right', 'left', 'forward', 'red', 'left') Optimal action: None Q: {'forward': -8.55, 'right': 0.0, None: 1.95, 'left': -9.55} Agent optimal? True
State: ('right', 'forward', None, 'red', 'right') Optimal action: right Q: {'forward': -38.6, 'right': 1.79, None: 1.37, 'left': -39.41} Agent optimal? True
State: (None, None, 'right', 'red', 'right') Optimal action: right Q: {'forward': -10.14, 'right': 1.44, None: 1.75, 'left': -39.81} Agent optimal? False
State: ('left', 'right', 'right', 'red', 'forward') Optimal action: None Q: {'forward': 0.0, 'right': 0.54, None: 0.0, 'left': -36.27} Agent optimal? False
State: (None, 'forward', None, 'red', 'left') Optimal action: None Q: {'forward': -40.36, 'right': 0.56, None: 1.81, 'left': -39.6} Agent optimal? True
State: ('forward', None, None, 'red', 'right') Optimal action: None Q: {'forward': -39.9, 'right': -19.92, None: 1.96, 'left': -39.91} Agent optimal? True
State: ('right', None, 'forward', 'red', 'left') Optimal action: None Q: {'forward': -10.24, 'right': 0.76, None: 1.81, 'left': -10.1} Agent optimal? True
State: ('forward', None, 'right', 'red', 'right') Optimal action: None Q: {'forward': -40.32, 'right': -19.85, None: 1.87, 'left': -33.49} Agent optimal? True
State: (None, 'left', 'left', 'red', 'forward') Optimal action: None Q: {'forward': -9.96, 'right': 0.87, None: 1.73, 'left': -9.65} Agent optimal? True
State: ('forward', 'right', None, 'red', 'forward') Optimal action: None Q: {'forward': -34.71, 'right': -20.15, None: 1.76, 'left': -29.08} Agent optimal? True
State: ('left', 'right', 'left', 'red', 'forward') Optimal action: None Q: {'forward': -9.98, 'right': 0.79, None: 0.58, 'left': -9.75} Agent optimal? False
State: ('right', None, 'left', 'red', 'left') Optimal action: None Q: {'forward': -9.63, 'right': 0.1, None: 2.0, 'left': -10.61} Agent optimal? True
State: (None, 'forward', 'right', 'red', 'right') Optimal action: right Q: {'forward': -25.38, 'right': 1.71, None: 2.24, 'left': -39.39} Agent optimal? False
State: ('left', None, 'right', 'red', 'right') Optimal action: right Q: {'forward': -9.25, 'right': 1.48, None: 1.67, 'left': -38.14} Agent optimal? False
State: (None, 'right', 'forward', 'red', 'right') Optimal action: right Q: {'forward': -9.76, 'right': 1.12, None: 1.8, 'left': -9.85} Agent optimal? False
State: ('right', 'right', 'left', 'red', 'left') Optimal action: None Q: {'forward': -4.42, 'right': 0.75, None: 0.0, 'left': 0.0} Agent optimal? False
State: ('forward', 'right', 'right', 'red', 'right') Optimal action: None Q: {'forward': 0.0, 'right': -1.02, None: 0.0, 'left': 0.0} Agent optimal? False
State: ('forward', 'right', 'left', 'red', 'left') Optimal action: None Q: {'forward': 0.0, 'right': -17.79, None: 1.58, 'left': -39.08} Agent optimal? True
State: ('forward', 'left', 'forward', 'red', 'left') Optimal action: None Q: {'forward': -40.22, 'right': -19.68, None: 1.78, 'left': -40.05} Agent optimal? True
State: (None, 'left', None, 'red', 'left') Optimal action: None Q: {'forward': -9.79, 'right': 0.83, None: 1.83, 'left': -9.91} Agent optimal? True
State: (None, 'left', 'right', 'red', 'forward') Optimal action: None Q: {'forward': -9.41, 'right': 0.47, None: 1.9, 'left': -40.51} Agent optimal? True
State: (None, 'right', None, 'red', 'forward') Optimal action: None Q: {'forward': -10.28, 'right': 0.4, None: 1.77, 'left': -10.27} Agent optimal? True
State: ('left', None, 'left', 'red', 'left') Optimal action: None Q: {'forward': -10.34, 'right': 0.78, None: 1.86, 'left': -10.44} Agent optimal? True
State: ('left', 'forward', 'right', 'red', 'right') Optimal action: right Q: {'forward': -28.17, 'right': 0.0, None: 1.9, 'left': 0.0} Agent optimal? False
State: ('left', None, 'forward', 'red', 'right') Optimal action: right Q: {'forward': -10.18, 'right': 1.38, None: 1.83, 'left': -10.07} Agent optimal? False
State: ('right', 'right', 'forward', 'red', 'right') Optimal action: right Q: {'forward': 0.0, 'right': 1.82, None: 0.0, 'left': -9.07} Agent optimal? True
State: ('right', None, 'forward', 'red', 'forward') Optimal action: None Q: {'forward': -9.53, 'right': 1.06, None: 1.75, 'left': -10.05} Agent optimal? True
State: (None, 'forward', 'forward', 'red', 'right') Optimal action: right Q: {'forward': -39.77, 'right': 1.86, None: 1.55, 'left': -40.14} Agent optimal? True
State: ('forward', 'forward', 'right', 'red', 'right') Optimal action: None Q: {'forward': -25.47, 'right': -19.42, None: 1.41, 'left': -36.52} Agent optimal? True
State: (None, 'forward', 'left', 'red', 'right') Optimal action: right Q: {'forward': -39.72, 'right': 1.79, None: 1.25, 'left': -39.57} Agent optimal? True
State: (None, 'right', 'right', 'red', 'forward') Optimal action: None Q: {'forward': -9.73, 'right': 0.82, None: 1.57, 'left': -30.33} Agent optimal? True
State: ('right', 'right', 'right', 'red', 'right') Optimal action: right Q: {'forward': 0.0, 'right': 0.14, None: 0.0, 'left': 0.0} Agent optimal? True
State: (None, None, None, 'red', 'forward') Optimal action: None Q: {'forward': -9.89, 'right': 1.16, None: 1.87, 'left': -9.97} Agent optimal? True
State: (None, 'right', None, 'red', 'right') Optimal action: right Q: {'forward': -9.89, 'right': 1.23, None: 2.12, 'left': -9.34} Agent optimal? False
State: ('right', 'forward', None, 'red', 'forward') Optimal action: None Q: {'forward': -38.31, 'right': 0.63, None: 1.98, 'left': -39.93} Agent optimal? True
State: ('right', 'forward', 'forward', 'red', 'left') Optimal action: None Q: {'forward': 0.0, 'right': 0.67, None: 0.0, 'left': -23.62} Agent optimal? False
State: ('forward', None, 'left', 'red', 'left') Optimal action: None Q: {'forward': -39.79, 'right': -19.98, None: 2.06, 'left': -39.87} Agent optimal? True
State: ('left', 'left', 'left', 'red', 'right') Optimal action: right Q: {'forward': -5.48, 'right': 2.03, None: 1.9, 'left': -7.95} Agent optimal? True
State: ('left', 'right', None, 'red', 'right') Optimal action: right Q: {'forward': -9.31, 'right': 1.57, None: 0.97, 'left': -9.79} Agent optimal? True
State: ('forward', 'right', 'forward', 'red', 'left') Optimal action: None Q: {'forward': -27.42, 'right': -8.58, None: 1.87, 'left': -39.9} Agent optimal? True
State: (None, None, 'forward', 'red', 'forward') Optimal action: None Q: {'forward': -10.32, 'right': 1.14, None: 1.8, 'left': -10.03} Agent optimal? True
State: ('left', 'left', None, 'red', 'left') Optimal action: None Q: {'forward': -10.26, 'right': 0.9, None: 1.91, 'left': -9.97} Agent optimal? True
State: ('forward', 'left', 'right', 'red', 'left') Optimal action: None Q: {'forward': -40.51, 'right': -20.22, None: 1.69, 'left': -28.45} Agent optimal? True
Optimal policy followed in 156/192 matched states

Process finished with exit code 0
